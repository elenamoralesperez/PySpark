{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bb63c8",
   "metadata": {},
   "source": [
    "# EJERCICIO FINAL PYSPARK (DOCKER + KAFKA + PYSPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1787def",
   "metadata": {},
   "source": [
    "## Creamos la sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb533b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6c59754987e7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ProyectoIoT</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffa53de1d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"ProyectoIoT\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa335a",
   "metadata": {},
   "source": [
    "spark.readStream\n",
    "Indica que vamos a leer datos en modo streaming, es decir, datos que llegan continuamente.\n",
    "\n",
    ".format(\"kafka\")\n",
    "Le decimos a Spark que la fuente de datos de streaming será Kafka.\n",
    "\n",
    ".option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "Especifica la dirección del clúster Kafka al que conectarse.\n",
    "En este caso \"kafka\" es el nombre del contenedor Docker y 9092 es el puerto del broker.\n",
    "\n",
    ".option(\"subscribe\", \"sensores\")\n",
    "Indica el topic de Kafka del que queremos leer mensajes → \"sensores\".\n",
    "\n",
    ".option(\"startingOffsets\", \"latest\")\n",
    "Define desde qué punto empezar a leer:\n",
    "\n",
    "\"latest\" → solo recibe mensajes nuevos, enviados después de iniciar el streaming.\n",
    "\n",
    "\"earliest\" sería para leer todos los mensajes antiguos también.\n",
    "\n",
    ".load()\n",
    "Ejecuta la configuración y crea un DataFrame de streaming, donde cada fila representa un mensaje recibido desde Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfb9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")  # <-- nombre del contenedor\n",
    "         .option(\"subscribe\", \"sensores\")\n",
    "         .option(\"startingOffsets\", \"latest\")\n",
    "         .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b03dec",
   "metadata": {},
   "source": [
    "Este bloque transforma los mensajes de Kafka en un formato que Spark puede procesar:\n",
    "\n",
    "Define la estructura de los datos que esperamos recibir (sensor, valor, temperatura, humedad, estado, timestamp y UUID).\n",
    "\n",
    "Convierte el JSON recibido en columnas individuales para poder trabajar con cada campo.\n",
    "\n",
    "Crea una columna de tiempo (event_time) a partir del timestamp original para poder usarlo en ventanas y agregaciones temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf795208",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"humidity\", DoubleType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"uuid\", StringType())\n",
    "])\n",
    "\n",
    "df = raw_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df = df.withColumn(\"event_time\", col(\"timestamp\").cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b512f5b",
   "metadata": {},
   "source": [
    "Este bloque realiza agregaciones por ventana de tiempo sobre el DataFrame de streaming:\n",
    "\n",
    "Define un watermark de 1 minuto para indicar a Spark hasta qué punto los datos antiguos se pueden considerar válidos. Esto ayuda a manejar retrasos y datos tardíos.\n",
    "\n",
    "Agrupa los datos cada 30 segundos por sensor (sensor_id).\n",
    "\n",
    "Calcula estadísticas dentro de cada ventana: promedio de valor, temperatura y humedad, y cuenta el número de eventos recibidos.\n",
    "\n",
    "El resultado es un DataFrame de streaming con resúmenes temporales por sensor listo para análisis o escritura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c99f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = (\n",
    "    df\n",
    "    .withWatermark(\"event_time\", \"1 minute\")   # ← IMPORTANTE\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"30 seconds\"),\n",
    "        col(\"sensor_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"value\").alias(\"avg_value\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "        count(\"*\").alias(\"num_events\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc852c82",
   "metadata": {},
   "source": [
    "Este bloque escribe los resultados del streaming en archivos Parquet de manera continua:\n",
    "\n",
    "Define la carpeta de salida donde se guardarán los archivos Parquet (resultados/).\n",
    "\n",
    "Usa un checkpoint (chk/) para que Spark recuerde qué datos ya procesó y pueda reiniciar de forma segura en caso de fallo.\n",
    "\n",
    "Modo append: los nuevos datos se agregan continuamente a los archivos existentes.\n",
    "\n",
    "Inicia el streaming, haciendo que las agregaciones se escriban en tiempo real mientras llegan nuevos datos.\n",
    "\n",
    "El resultado es un conjunto de archivos Parquet que refleja las métricas agregadas por ventana y por sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc810a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_query = (\n",
    "    agg_df\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", \"resultados/\")\n",
    "    .option(\"checkpointLocation\", \"chk/\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dd9b4",
   "metadata": {},
   "source": [
    "A partir de aquí, te toca a ti, sigue las cuestiones planteadas en el Readme y completa el notebook. Despues guardatelo con los outputs de las celdas y súbelo al repo. Mucha suerte que ya lo teneis ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d4ab4",
   "metadata": {},
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2001725",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414fd44",
   "metadata": {},
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d297ac2",
   "metadata": {},
   "source": [
    "### Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae0206",
   "metadata": {},
   "source": [
    "### Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d485e",
   "metadata": {},
   "source": [
    "### Ejercicio 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b872278",
   "metadata": {},
   "source": [
    "### Ejercicio 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffb24b",
   "metadata": {},
   "source": [
    "### Ejercicio 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739c8e6",
   "metadata": {},
   "source": [
    "### Ejercicio 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226af8c",
   "metadata": {},
   "source": [
    "### Ejercicio 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb548717",
   "metadata": {},
   "source": [
    "### Ejercicio 11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
